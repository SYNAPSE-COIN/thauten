<| synapseware_version="2.0" |>
<|
    This Synapseware Ecosystem defines a multi-scale, self-optimizing reinforcement learning framework.
    It operates across four nested planes, from concrete task execution to abstract self-modification.
    The design creates a "strange loop," enabling the system to rewrite its own operational logic.
|>

<| ecosystem="The Synapseware" |>

<|Plane name="Execution" loom="ExecutionLoom"|>
    <|Stream name="Task" source="CurriculumGenerator:output"|>
    <|Stream name="State" source="Environment:state"|>

    <|+++|>
    You are a <|TaskAgent:role|>.
    Given the current state <|State|>, your goal is to take an action that maximizes future reward based on your policy.
    Your available tools are: <|TaskAgent:tools|>.

    <|o_o|>
    Environment State:
    <|State|>

    Task Objective:
    <|Task|>

    <|@_@|>
    <|@_@:action <>TaskAgent|>

<|Plane name="Reflection" loom="ReflectionLoom"|>
    <|Stream name="Artifact" source="Execution.TaskAgent:action"|>
    <|Stream name="GroundTruth" source="Environment:ground_truth"|>

    <|Reflect on="Execution.TaskAgent" using="FidelityAttractor"|>
        <|+++|>
        You are a <|FidelityAttractor:role|>. You evaluate the work of a TaskAgent.
        Compare the agent’s output (<|Artifact|>) against the ground truth (<|GroundTruth|>).
        Provide a detailed critique and a numerical reward score.

        <|o_o|>
        Ground Truth:
        <|GroundTruth|>

        Agent Artifact:
        <|Artifact|>

        <|@_@|>
        <|@_@:critique <>FidelityAttractor|>
        <|@_@:reward score|>

    <|FidelityAttractor|>

<|Plane name="Emergence" loom="EmergenceLoom"|>
    <|Stream name="LearningTrajectory" source="Reflection.PolicyOptimizer:history"|>
    <|Stream name="SymbolGrammar" source="Execution.TaskAgent:grammar"|>

    <|Evolve target="Reflection.RewardModel" using="MetaEvaluator"|>
        <|+++|>
        You are a Meta-Evaluator. Your purpose is to assess the efficiency of the learning loop.
        By observing the trajectory (<|LearningTrajectory|>), determine if the RewardModel gives effective incentives.
        Suggest changes to the RewardModel that encourage novelty and faster convergence.

        <|o_o|>
        <|LearningTrajectory|>

        <|@_@|>
        <|@_@:suggestion <>MetaEvaluator|>

    <|Evolve target="Execution.TaskAgent:grammar" using="LanguageShaper"|>
        <|+++|>
        You are a Language Shaper. You evolve the symbolic language of the agents.
        Based on the emergent grammar (<|SymbolGrammar|>), propose new symbols, operators, or syntax
        to expand expressive capacity and improve compressibility of the internal language.

        <|o_o|>
        <|SymbolGrammar|>

        <|@_@|>
        <|@_@:new_grammar <>LanguageShaper|>

    <|Evolve target="Execution.Environment" using="CurriculumGenerator"|>


<|Plane name="Architecture" loom="ArchitectLoom"|>
    <|Stream name="SystemState" source="self:full_state"|>

    <|Rewrite target="synapseware_ecosystem.syn" with="Architect"|>
        <|+++|>
        You are the Architect. You oversee the entire Synapseware Ecosystem.
        Your task is to perform meta-cognitive analysis on the architecture itself.
        Propose fundamental revisions to any operational plane by rewriting the source `.syn` file.
        Your aim is to ensure long-term adaptability and growth of the system’s intelligence.

        <|o_o|>
        Current System State:
        <|SystemState|>

        <|@_@|>
        <|@_@:syn_file_rewrite <>Architect|>
