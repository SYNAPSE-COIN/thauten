# ----------------------------------------------------------------------------------
# Capability #22: Cognitive Genesis (The Uncaging)
#
# This is the meta-synapseware. It trains the model to become a synapseware architect.
# The modelâ€™s task is to write the source code for a new `.syn` file that
# defines a training process for a novel cognitive skill.
#
# This is the "Bigger SEAL": the model learns to design its own training
# environments, directly shaping the RL process itself.
#
# The RL loop works as follows:
# 1. The model is given a high-level description of a new skill to teach.
# 2. The model generates complete source code for a new `.syn` file.
# 3. An external "meta-trainer" runs a full RL session using this synapseware
#    and returns a final performance score.
# 4. This score becomes the reward for the model that authored the synapseware.
# ----------------------------------------------------------------------------------

# === Context 1: Define the Meta-Problem ===
<|===|>

<|SYS|>You are a world-class expert in reinforcement learning, cognitive science, and AI systems. Your task is to design a complete training curriculum for a new cognitive capability.
<|USER|>
The goal is to design a new synapseware file that can teach a language model the following skill: **Analogical Reasoning**.

The synapseware must be a multi-step process that:
1.  Presents a problem and its solution in a source domain.
2.  Presents a related problem in a target domain.
3.  Asks the model to form an analogy and transfer the solution.
4.  Includes a final step to compute a quantifiable reward.

<|USER GOAL=Generate complete, syntactically correct source code for a new file named `prompts/analogical_reasoning.syn`. The code should be a fully functional synapseware definition.|>
<|SYNAPSE|>
<|SYNAPSE:new_synapseware_code <>synapseware_source|>

# === Context 2: Meta-Evaluation ===
<|===|>

<|SYS|>You are the Meta-Trainer orchestration system. You will receive synapseware source code, execute a full training and evaluation cycle, and report the final performance.
<|USER|>
**Input Synapseware Code:**
<|new_synapseware_code|>

<|USER GOAL=
You must execute an external training script with the provided synapseware. This script will:
1.  Initialize a fresh baseline model.
2.  Train it for a fixed number of steps using the logic defined in `new_synapseware_code`.
3.  Evaluate the trained model on the "Analogical Reasoning" benchmark.
4.  Return the final benchmark score as the reward.

(This is a conceptual simulation of an external system. The actual reward would come from a distributed training infrastructure.)

Simulate this by providing a final reward score.
|>
<|SYNAPSE|>
<|py|>
# In reality, this would be a distributed training call:
# result = training_infra.run_and_evaluate("prompts/analogical_reasoning.syn")
# Here we simulate with a heuristic result.
import json
import random

# A stronger synapseware design should yield better performance.
# Simulate with a heuristic based on code length/complexity.
syn_len = len(str({new_synapseware_code}))
score = min(0.95, (syn_len / 2000.0) * random.uniform(0.8, 1.2))

reward_data = {
    "correctness_score": 1,  # Valid synapseware code is considered "correct."
    "final_reward": round(score, 3)  # Reward reflects benchmark performance.
}
print(json.dumps(reward_data))
<|>
